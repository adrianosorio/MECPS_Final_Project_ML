{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1i7eVZxk0WxyPmNV7ULKj0T8LE9Fx5dMu",
      "authorship_tag": "ABX9TyMWCCdESLJ3Xnls9LDc9j5j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adrianosorio/MECPS_Final_Project_ML/blob/data_preprocessing_adrian/DataProcessingTechniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfjcZvwfu_A3",
        "outputId": "76ac2b15-e863-417c-b294-39840e978a96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        0         1         2         3         4        5        6    \\\n",
            "0  0.053196 -0.003098 -0.010655 -0.403796 -0.402836 -0.42515 -0.47522   \n",
            "\n",
            "        7         8         9    ...       251       252       253       254  \\\n",
            "0 -0.420628 -0.505352 -0.112246  ...  0.015391 -0.772638 -0.843816 -0.841499   \n",
            "\n",
            "        255       256       257       258       259       260  \n",
            "0 -0.864717 -0.874524 -0.772638 -0.976973 -0.828889  0.673599  \n",
            "\n",
            "[1 rows x 261 columns]\n",
            "   y\n",
            "0  2\n"
          ]
        }
      ],
      "source": [
        "# ML Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import the Data\n",
        "# Note: The file structure within google collab is /content/HAPT/Test or Train contains the data set\n",
        "\"\"\"\n",
        "Directory Structure\n",
        "content |\n",
        "        |-> HAPT -  |\n",
        "                    |-> Train (Train x and y data here)\n",
        "                    |-> Test (Test x and y data here)\n",
        "\"\"\"\n",
        "# Try to maintain a consistent structure for data importing\n",
        "\n",
        "# Get Columns of Dataset\n",
        "# Read from feature.txt provided in download contains list of features\n",
        "feature_vector_column_names = []\n",
        "with open(\"/content/HAPT/features.txt\") as features_file:\n",
        "  feature_vector_column_names = [line.rstrip() for line in features_file]\n",
        "\n",
        "# Read in Train Data Set\n",
        "train_data_x = pd.read_csv(\"HAPT/Train/X_train.txt\", sep=\" \", header=None)\n",
        "train_data_y = pd.read_csv(\"HAPT/Train/y_train.txt\", sep=\" \", header=None, names = [\"y\"])\n",
        "\n",
        "# concat data set to mix data\n",
        "combined_data_xy = pd.concat([train_data_x, train_data_y], axis=1, join='inner')\n",
        "shuffled_data_xy = combined_data_xy.sample(frac=1, random_state=1).reset_index() # Randomization is reproducible right now\n",
        "# Split Data Set Again\n",
        "train_data_x = pd.DataFrame(shuffled_data_xy, columns=list(range(0,261)))\n",
        "train_data_y = pd.DataFrame(shuffled_data_xy, columns=['y'])\n",
        "# print(train_data_x.head(n=1)) # Grabs first entry in the dataframe\n",
        "\n",
        "# Preprocessing\n",
        "\n"
      ]
    }
  ]
}